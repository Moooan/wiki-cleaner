# Wiki Article Cleaner

## 项目简介

本项目旨在对原始维基百科 XML 数据进行高质量中文文本清洗与结构化处理，生成适用于中文自然语言处理（NLP）任务的规范化数据集。

清洗流程包括：
- 解析并清理 Wiki 文本
- 模板内容智能解析与保留（如出生日期、语言名等）
- 图片、表格、引用、无关模板等冗余信息删除
- 繁体转简体
- 中文标点与英文标点统一
- 删除长段英文文本
- 保持必要的元数据信息（如标题、编辑者、时间戳等）
- 等等

最终输出格式为：
- 每条数据包含一段文本 (`text`) 和其对应的元信息 (`meta`)。
- 支持 JSON 文件输出，每行一个条目。

---

## 目录结构

```bash
wiki_cleaner/
├── handlers/
│   └── download_data.py          # 在制定网页中下载数据
│   └── wiki_xml_handler.py       # 自定义 SAX 解析器
├── text_cleaner/
│   └── template_cleaner.py       # 模板节点处理逻辑
│   └── wiki_text_cleaner.py      # 解析并清理 Wiki 文本
│   └── simplified_words.py       # 将繁体中文转化成简体中文
│   └── refine_cleaner.py         # 二次细粒度清洗（段落断开、空格处理等）
│   └── simbol_cleaner.py         # 解析并清理多种符号（表格、层级符号等）
│   └── en_cleaner.py             # 清理含有大量英文的段落
│   └── processor.py              # 进行数据清洗总流程
├── data/
│   └── wiki_articles_final_cleaned.json
├── README.md
├── requirements.txt
└── main.py                       # 主程序，整合调用以上模块
```

## 遇到的问题

- 关于标题的问题，==XX====YY==，在这里用普通方法识别的时候，一直会识别错，只删掉==XX====，后来采用前后删掉一样数量的“=”就可以轻松解决问题。一开始想要删掉所有的标题，因为有的标题下因为是链接，在数据清洗的时候会被删掉，会导致标题下没有正文的情况。后来还是保留了标题但是进行识别操作，识别标题下是否有正文再决定是否删除标题。

- * \# 这种符号不确定是否删除，因为* 和\# 在维基百科文章中有多重含义：强调、打标签、表示结构的层级、表示有序和无序列表等等，在不同情况下怎么处理不同的符号，是需要花多一点的时间来套路的

- 关于文章的template，维基百科文章中有非常多的template，有的其中会包含关键信息，比如英文名、出生年月、转接链接，这些信息在句子中是有用信息，不能直接删除，但是有的template表示的是JAVA的格式、html网页本身的注释等等，甚至在维基百科文章内部也可以自己定义不同的模版，比如军事模块等等。不同的template处理起来会比较麻烦。另外，还有类似{{PRC admin/navcat\|41/03/29/200/000}} 这种模版，专门用于中国行政区划页面的分类和导航。如果不删除，需要自行查找信息对应；如果删除会导致句子成分缺失。
